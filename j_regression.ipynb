{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up a neural network regression using Keras with a dataset that is stored in a JSON file as a list of objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 21:42:20.936614: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 21:42:21.351935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:21.351965: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 21:42:23.212515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:23.212697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:23.212725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 21:42:25.799330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-09 21:42:25.799969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800115: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800228: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800339: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800447: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800554: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-09 21:42:25.800788: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-09 21:42:25.801416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 39), found shape=(None, 34)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[1;32m     73\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     74\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebr3o6pu2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nikitich/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 39), found shape=(None, 34)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('user_input/scenarios.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "X = []\n",
    "y = []\n",
    "for example in data:\n",
    "    X.append([\n",
    "        example['weather'],\n",
    "        example['vehicle'],\n",
    "        example['traffic'],\n",
    "        example['emergency'],\n",
    "        example['timeOfDay'],\n",
    "        example['location'],\n",
    "        example['intersections'],\n",
    "        example['pedestrians'],\n",
    "        example['pedestrian_cross'],\n",
    "        example['route_length'],\n",
    "\n",
    "    ])\n",
    "    y.append(example['total_difficulty_rating'])\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('user_input/scenarios.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# One-hot encode the features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a list of indices corresponding to the categorical columns in X\n",
    "cat_cols = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize the one-hot encoder\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "\n",
    "# Fit the encoder on X\n",
    "encoder.fit(X)\n",
    "\n",
    "# Transform X using the encoder\n",
    "X_encoded = encoder.transform(X)\n",
    "\n",
    "# Print the shape of the encoded X\n",
    "print(X_encoded.shape)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X = X_encoded \n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(39,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions with the model\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate new scenarios using the predictions from the neural network, you can use the predicted values as input for the parameters of the new scenarios. Here's an example of how you can do that:\n",
    "\n",
    "Get the predicted values for each scenario from the neural network model.\n",
    "Scale the predicted values back to their original range if you normalized the data during training.\n",
    "Use the predicted values to generate new scenarios with higher difficulty scores. For example, you can increase the number of cars or pedestrians, or set the weather to a more challenging condition.\n",
    "Save the new scenarios to a JSON file.\n",
    "\n",
    "Here's an example of how you can generate new scenarios based on the predicted values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "predicted_values = predictions\n",
    "# Scale the predicted values back to their original range if needed\n",
    "\n",
    "# Generate new scenarios based on the predicted values\n",
    "new_scenarios = []\n",
    "for i in range(len(predicted_values)):\n",
    "    # Increase the number of cars and pedestrians based on the predicted values\n",
    "    num_cars = int(predicted_values[i][0] * 10) # Scale the value from 0-1 to 0-10\n",
    "    pedestrians = True if predicted_values[i][1] > 0.5 else False # Set pedestrians to True if the value is above 0.5\n",
    "    # Set other parameters to a fixed value or use the predicted values if appropriate\n",
    "    scenario = {\n",
    "        'weather': 'Rain',\n",
    "        'road': 'City',\n",
    "        'vehicle': 'Car',\n",
    "        'traffic': 'Heavy',\n",
    "        'emergency': 'No',\n",
    "        'timeOfDay': 'Day',\n",
    "        'location': 'Urban',\n",
    "        \"num_cars\": num_cars,\n",
    "        \"intersection\": True,\n",
    "        \"pedestrians\": pedestrians,\n",
    "        \"pedestrian_cross\": True,\n",
    "    }\n",
    "    new_scenarios.append(scenario)\n",
    "\n",
    "# Save the new scenarios to a JSON file\n",
    "with open('new_scenarios.json', 'w') as f:\n",
    "    json.dump(new_scenarios, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of potential values for each feature\n",
    "weather = ['Sunny', 'Rain', 'Thunderstorm']\n",
    "vehicle = ['Small', 'Truck','Van']\n",
    "traffic = ['Heavy', 'Light','Medium']\n",
    "emergency = ['Yes', 'No']\n",
    "timeOfDay = ['Day', 'Night','Dawn','Dusk']\n",
    "location = ['Urban', 'Country','Downtown']\n",
    "intersections = [0,1,2,3,4]\n",
    "pedestrians = [True,False]\n",
    "pedestrian_cross = [True, False]\n",
    "route_length = [100,200,300]\n",
    "\n",
    "\n",
    "# Generate three new scenarios with high difficulty scores\n",
    "for i in range(3):\n",
    "    # Randomly select values for each feature\n",
    "    new_scenario = [\n",
    "        np.random.choice(weather),\n",
    "        np.random.choice(vehicle),\n",
    "        np.random.choice(traffic),\n",
    "        np.random.choice(timeOfDay),\n",
    "        np.random.choice(location),\n",
    "        np.random.choice(intersections),\n",
    "        np.random.choice(route_length)\n",
    "    ]\n",
    "    \n",
    "    # One-hot encode the new scenario\n",
    "    new_scenario_encoded = encoder.transform([new_scenario])\n",
    "    \n",
    "    # Predict the difficulty rating for the new scenario\n",
    "    difficulty_score = model.predict(new_scenario_encoded)[0][0]\n",
    "    \n",
    "    # Print the new scenario and its difficulty score\n",
    "    print(f\"New scenario {i+1}: {new_scenario} - Difficulty score: {difficulty_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
