{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up a neural network regression using Keras with a dataset that is stored in a JSON file as a list of objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30)\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 60777.9609 - val_loss: 111990.1094\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60776.7266 - val_loss: 111988.3828\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60775.5703 - val_loss: 111986.6797\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60774.4648 - val_loss: 111985.0391\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 60773.3672 - val_loss: 111983.5156\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60772.2812 - val_loss: 111981.9844\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60771.2500 - val_loss: 111980.4766\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 60770.2656 - val_loss: 111978.9922\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60769.3164 - val_loss: 111977.5312\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60768.3906 - val_loss: 111976.1016\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 60767.5000 - val_loss: 111974.7578\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60766.7109 - val_loss: 111973.4453\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60765.9531 - val_loss: 111972.1797\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60765.1953 - val_loss: 111970.9297\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60764.4375 - val_loss: 111969.7109\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 60763.6875 - val_loss: 111968.5234\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60762.9453 - val_loss: 111967.3438\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60762.2188 - val_loss: 111966.1953\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60761.5430 - val_loss: 111965.0703\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 60760.8711 - val_loss: 111963.9688\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 60760.1953 - val_loss: 111962.9844\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 60759.5117 - val_loss: 111962.2422\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60758.8281 - val_loss: 111961.4609\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60758.1406 - val_loss: 111960.6953\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60757.4453 - val_loss: 111959.9219\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60756.7422 - val_loss: 111959.1328\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60756.0156 - val_loss: 111958.3594\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60755.2500 - val_loss: 111957.5781\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60754.4609 - val_loss: 111956.7891\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60753.6641 - val_loss: 111955.9766\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 60752.8320 - val_loss: 111955.1484\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60751.9688 - val_loss: 111954.3281\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60751.0898 - val_loss: 111953.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60750.1641 - val_loss: 111952.5781\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60749.2148 - val_loss: 111951.5078\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60748.2344 - val_loss: 111950.4062\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60747.2188 - val_loss: 111949.2812\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60746.1055 - val_loss: 111948.1328\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60744.9453 - val_loss: 111946.9531\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60743.7422 - val_loss: 111945.7656\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60742.5039 - val_loss: 111944.5859\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60741.2344 - val_loss: 111943.3828\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60739.9258 - val_loss: 111942.1250\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 60738.5703 - val_loss: 111940.8906\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60737.1719 - val_loss: 111939.6875\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60735.7344 - val_loss: 111938.4531\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60734.1875 - val_loss: 111937.1875\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 60732.5898 - val_loss: 111935.8750\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60730.9375 - val_loss: 111934.5469\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 60729.1406 - val_loss: 111933.1719\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60727.2422 - val_loss: 111931.7344\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60725.2539 - val_loss: 111930.2734\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60723.1875 - val_loss: 111928.7734\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 60721.0430 - val_loss: 111927.2266\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60718.8242 - val_loss: 111925.6484\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60716.5234 - val_loss: 111924.0312\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 60713.9609 - val_loss: 111922.3828\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60711.2578 - val_loss: 111920.6875\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 60708.4414 - val_loss: 111918.9453\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 60705.4961 - val_loss: 111916.2344\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60702.4219 - val_loss: 111913.4219\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 60699.2383 - val_loss: 111910.5234\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60696.0078 - val_loss: 111907.1328\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 60692.7109 - val_loss: 111903.0625\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60689.3047 - val_loss: 111898.9375\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 60685.7891 - val_loss: 111894.8828\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60682.1367 - val_loss: 111890.6797\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60678.3984 - val_loss: 111886.3516\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 60674.5938 - val_loss: 111881.9219\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60670.6758 - val_loss: 111877.3672\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60666.6406 - val_loss: 111872.7188\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 60662.6016 - val_loss: 111867.9609\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 60658.5195 - val_loss: 111863.1250\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60654.3281 - val_loss: 111858.1719\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60650.0781 - val_loss: 111853.0547\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60645.7422 - val_loss: 111847.7344\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60641.3203 - val_loss: 111842.2969\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60636.7656 - val_loss: 111836.7109\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 60632.0469 - val_loss: 111831.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60627.1992 - val_loss: 111825.1719\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60622.2344 - val_loss: 111819.1562\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 60617.1211 - val_loss: 111813.0703\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 60611.8477 - val_loss: 111806.9141\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 60606.4062 - val_loss: 111800.5938\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 60600.8203 - val_loss: 111794.1094\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 60595.0859 - val_loss: 111787.4141\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 60589.1875 - val_loss: 111780.5156\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60583.1289 - val_loss: 111773.4688\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60576.9062 - val_loss: 111766.2500\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60570.5273 - val_loss: 111758.8359\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60563.9727 - val_loss: 111751.2344\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 60557.2188 - val_loss: 111743.4141\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60550.1992 - val_loss: 111735.3750\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 60542.9922 - val_loss: 111727.0938\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 60535.5781 - val_loss: 111718.5625\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 60527.8828 - val_loss: 111709.8047\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 60519.9453 - val_loss: 111700.8125\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 60511.6133 - val_loss: 111691.2734\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 60502.9492 - val_loss: 111681.3125\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 60493.9688 - val_loss: 111671.0469\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 111671.0469\n",
      "Test loss: 111671.046875\n",
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('user_input/scenarios.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "X = []\n",
    "y = []\n",
    "for example in data:\n",
    "    X.append([\n",
    "        example['weather'],\n",
    "        example['vehicle'],\n",
    "        example['traffic'],\n",
    "        example['timeOfDay'],\n",
    "        example['location'],\n",
    "        example['intersections'],\n",
    "        example['route_length'],\n",
    "\n",
    "    ])\n",
    "    y.append(example['total_difficulty_rating'])\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('user_input/scenarios.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# One-hot encode the features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a list of indices corresponding to the categorical columns in X\n",
    "cat_cols = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize the one-hot encoder\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "\n",
    "# Fit the encoder on X\n",
    "encoder.fit(X)\n",
    "\n",
    "# Transform X using the encoder\n",
    "X_encoded = encoder.transform(X)\n",
    "\n",
    "# Print the shape of the encoded X\n",
    "print(X_encoded.shape)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X = X_encoded \n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(30,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions with the model\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate new scenarios using the predictions from the neural network, you can use the predicted values as input for the parameters of the new scenarios. Here's an example of how you can do that:\n",
    "\n",
    "Get the predicted values for each scenario from the neural network model.\n",
    "Scale the predicted values back to their original range if you normalized the data during training.\n",
    "Use the predicted values to generate new scenarios with higher difficulty scores. For example, you can increase the number of cars or pedestrians, or set the weather to a more challenging condition.\n",
    "Save the new scenarios to a JSON file.\n",
    "\n",
    "Here's an example of how you can generate new scenarios based on the predicted values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# # Load the predicted values from the neural network model\n",
    "# with open('predicted_values.json', 'r') as f:\n",
    "#     predicted_values = json.load(f)\n",
    "\n",
    "predicted_values = predictions\n",
    "# Scale the predicted values back to their original range if needed\n",
    "\n",
    "# Generate new scenarios based on the predicted values\n",
    "new_scenarios = []\n",
    "for i in range(len(predicted_values)):\n",
    "    # Increase the number of cars and pedestrians based on the predicted values\n",
    "    num_cars = int(predicted_values[i][0] * 10) # Scale the value from 0-1 to 0-10\n",
    "    pedestrians = True if predicted_values[i][1] > 0.5 else False # Set pedestrians to True if the value is above 0.5\n",
    "    # Set other parameters to a fixed value or use the predicted values if appropriate\n",
    "    scenario = {\n",
    "        'weather': 'Rain',\n",
    "        'road': 'City',\n",
    "        'vehicle': 'Car',\n",
    "        'traffic': 'Heavy',\n",
    "        'emergency': 'No',\n",
    "        'timeOfDay': 'Day',\n",
    "        'location': 'Urban',\n",
    "        \"num_cars\": num_cars,\n",
    "        \"intersection\": True,\n",
    "        \"pedestrians\": pedestrians,\n",
    "        \"pedestrian_cross\": True,\n",
    "    }\n",
    "    new_scenarios.append(scenario)\n",
    "\n",
    "# Save the new scenarios to a JSON file\n",
    "with open('new_scenarios.json', 'w') as f:\n",
    "    json.dump(new_scenarios, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m scenarios\n\u001b[1;32m     37\u001b[0m \u001b[39m# Generate 10 new scenarios\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m new_scenarios \u001b[39m=\u001b[39m generate_scenarios(model, encoder, n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Print the new scenarios\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m scenario \u001b[39min\u001b[39;00m new_scenarios:\n",
      "Cell \u001b[0;32mIn [13], line 12\u001b[0m, in \u001b[0;36mgenerate_scenarios\u001b[0;34m(model, encoder, n)\u001b[0m\n\u001b[1;32m      9\u001b[0m input_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m7\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# One-hot encode the input vector\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m input_vec_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mtransform(input_vec)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Make a prediction with the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(input_vec_encoded)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:882\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[39m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m    878\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39min\u001b[39;00m {\n\u001b[1;32m    879\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minfrequent_if_exist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    881\u001b[0m }\n\u001b[0;32m--> 882\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[1;32m    883\u001b[0m     X,\n\u001b[1;32m    884\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    885\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    886\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[1;32m    887\u001b[0m )\n\u001b[1;32m    888\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[1;32m    890\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:152\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[1;32m    151\u001b[0m     Xi \u001b[39m=\u001b[39m X_list[i]\n\u001b[0;32m--> 152\u001b[0m     diff, valid_mask \u001b[39m=\u001b[39m _check_unknown(Xi, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategories_[i], return_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    154\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(valid_mask):\n\u001b[1;32m    155\u001b[0m         \u001b[39mif\u001b[39;00m handle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[0;34m(values, known_values, return_mask)\u001b[0m\n\u001b[1;32m    300\u001b[0m         valid_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(values), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[39m# check for nans in the known_values\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49misnan(known_values)\u001b[39m.\u001b[39many():\n\u001b[1;32m    304\u001b[0m     diff_is_nan \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(diff)\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m diff_is_nan\u001b[39m.\u001b[39many():\n\u001b[1;32m    306\u001b[0m         \u001b[39m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# Define a function to generate new scenarios based on the predictions\n",
    "def generate_scenarios(model, encoder, n=10):\n",
    "    # Create a list to store the generated scenarios\n",
    "    scenarios = []\n",
    "    \n",
    "    # Generate n scenarios\n",
    "    for i in range(n):\n",
    "        # Create a random input vector\n",
    "        input_vec = np.random.rand(1, 7)\n",
    "        \n",
    "        # One-hot encode the input vector\n",
    "        input_vec_encoded = encoder.transform(input_vec)\n",
    "        \n",
    "        # Make a prediction with the model\n",
    "        prediction = model.predict(input_vec_encoded)\n",
    "        \n",
    "        # Convert the prediction to a difficulty rating\n",
    "        difficulty_rating = prediction[0][0]\n",
    "        \n",
    "        # Create a new scenario dictionary\n",
    "        scenario = {\n",
    "            'weather': 'sunny',\n",
    "            'vehicle': 'car',\n",
    "            'traffic': 'light',\n",
    "            'timeOfDay': 'day',\n",
    "            'location': 'urban',\n",
    "            'intersections': 2,\n",
    "            'route_length': 5.0,\n",
    "            'total_difficulty_rating': difficulty_rating\n",
    "        }\n",
    "        \n",
    "        # Add the scenario to the list\n",
    "        scenarios.append(scenario)\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Generate 10 new scenarios\n",
    "new_scenarios = generate_scenarios(model, encoder, n=10)\n",
    "\n",
    "# Print the new scenarios\n",
    "for scenario in new_scenarios:\n",
    "    print(scenario)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
